{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Library & Input data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom datetime import datetime\nfrom unidecode import unidecode\nfrom itertools import combinations\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport category_encoders as ce\n\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/klps-creditscring-challenge-for-students/test.csv\n/kaggle/input/klps-creditscring-challenge-for-students/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/klps-creditscring-challenge-for-students/train.csv')\ntest = pd.read_csv('/kaggle/input/klps-creditscring-challenge-for-students/test.csv')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop some columns are duplicated, with correlation = NaN\nignore_columns = ([\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \n        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]] + \n        ['partner0_K', 'partner0_L', \n         'partner1_B', 'partner1_D', 'partner1_E', 'partner1_F', 'partner1_K', 'partner1_L',\n         'partner2_B', 'partner2_G', 'partner2_K', 'partner2_L',\n         'partner3_B', 'partner3_C', 'partner3_F', 'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L',\n         *['partner4_' + i for i in 'ABCDEFGHK'],\n         'partner5_B', 'partner5_C', 'partner5_H', 'partner5_K', 'partner5_L'])\n\n# Some auto columns could make new better columns\nall_auto_columns = list(set([c for c in train.columns if train[c].dtype in [np.int64, np.float64]])\n                    .difference(ignore_columns + ['currentLocationLocationId', 'homeTownLocationId', 'label', 'id']))\n\nauto_columns_1 = [c for c in all_auto_columns if 'Field_' in c]\nauto_columns_2 = [c for c in all_auto_columns if 'partner' in c]\nauto_columns_3 = [c for c in all_auto_columns if 'num' in c]\nauto_columns_4 = [c for c in all_auto_columns if c not in auto_columns_1 + auto_columns_2 + auto_columns_3]\nprint(len(auto_columns_1), len(auto_columns_2), len(auto_columns_3), len(auto_columns_4), len(all_auto_columns))","execution_count":3,"outputs":[{"output_type":"stream","text":"37 27 12 11 87\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Datetime columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 35, 40]]\ndatetime_cols = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\ncorrect_dt_cols = ['Field_34', 'ngaySinh']\ncat_cols = date_cols + datetime_cols + correct_dt_cols\n\n# Normalize Field_34, ngaySinh\ndef ngaysinh_34_normalize(s):\n    if s != s: return np.nan\n    try: s = int(s)\n    except ValueError: s = s.split(\" \")[0]\n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\n# Normalize datetime data\ndef datetime_normalize(s):\n    if s != s: return np.nan\n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\": s = s[:-1]\n    return datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n\n# Normalize date data\ndef date_normalize(s):\n    if s != s: return np.nan\n    try: t = datetime.strptime(s, \"%m/%d/%Y\")\n    except: t = datetime.strptime(s, \"%Y-%m-%d\")\n    return t\n\ndef process_datetime_cols(df):\n    df[datetime_cols] = df[datetime_cols].applymap(datetime_normalize)  \n    df[date_cols] = df[date_cols].applymap(date_normalize)\n    df[correct_dt_cols] = df[correct_dt_cols].applymap(ngaysinh_34_normalize)\n\n    # Some delta columns\n    for i, j in zip('43 1 2'.split(), '1 2 44'.split()): df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.seconds\n    for i, j in zip('5 6 7 33 8 11 9 15 25 6 7 8 9 15 25 2'.split(), '6 34 33 40 11 35 15 25 32 7 8 9 15 25 32 8'.split()): \n        df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.days\n    \n    # Age, month\n    df['age'] = 2020 - pd.DatetimeIndex(df['ngaySinh']).year\n    df['birth_month'] = pd.DatetimeIndex(df['ngaySinh']).month\n    \n    # Days from current time & isWeekday\n    for col in cat_cols:\n        name = col.split('_')[-1]\n        df[f'is_WD_{name}'] = df[col].dt.dayofweek.isin(range(5))\n        df[f'days_from_now_{name}'] = (datetime.now() - pd.DatetimeIndex(df[col])).days\n        df[col] = df[col].dt.strftime('%m-%Y')\n    \n    # Delta for x_startDate and x_endDate\n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n        \n        df[f'{cat}_start_end'] = (df[f'{cat}_endDate'] - df[f'{cat}_startDate']).dt.days\n        \n    for i, j in zip('F E C G'.split(), 'E C G A'.split()):\n        df[f'{j}_{i}_startDate'] = (df[f'{j}_startDate'] - df[f'{i}_startDate']).dt.days\n        df[f'{j}_{i}_endDate'] = (df[f'{j}_endDate'] - df[f'{i}_endDate']).dt.days\n    \n    temp_date = [f'{i}_startDate' for i in 'ACEFG'] + [f'{i}_endDate' for i in 'ACEFG']\n    \n    for col in temp_date:\n        df[col] = df[col].dt.strftime('%m-%Y')\n        \n    for col in cat_cols + temp_date:\n        df[col] = df[col]\n        \n    return df","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"unicode_cols = ['Field_18', 'maCv', 'diaChi', 'Field_46', 'Field_48', 'Field_49', 'Field_56', 'Field_61', 'homeTownCity', \n                'homeTownName', 'currentLocationCity', 'currentLocationName', 'currentLocationState', 'homeTownState']\nobject_cols = (unicode_cols + \n               [f'Field_{str(i)}' for i in '4 12 36 38 47 62 45 54 55 65 66 68'.split()] +\n               ['data.basic_info.locale', 'currentLocationCountry', 'homeTownCountry', 'brief'])\n\ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef combine_gender(s):\n    x, y = s \n    if x != x and y != y: return \"nan\"\n    if x != x: return y.lower()\n    return x.lower()\n\ndef process_categorical_cols(df):\n    df['diaChi'] = df['diaChi'].str.split(',').str[-1]\n    df[unicode_cols] = df[unicode_cols].applymap(str_normalize).applymap(lambda x: unidecode(x) if x==x else x)\n    \n    # Normalize some columns\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngoài quốc doanh Quận 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    # Make some new features\n    df['Field_45_Q'] = df['Field_45'].str[:-3].astype('category')\n    df['Field_45_TP_55'] = df['Field_45'].str[:2] == df['Field_55']\n    df['is_homeTown_diaChi'] = df['homeTownCity'] == df['diaChi']\n    df['is_homeTown_current_city'] = df['homeTownCity'] == df['currentLocationCity']\n    df['is_homeTown_current_state'] = df['homeTownState'] == df['currentLocationState']\n    df['F48_49'] = df['Field_48'] == df['Field_49']\n    \n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    \n    df[[\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n        \"homeTownLatitude\", \"homeTownLongitude\"]].replace(0, np.nan, inplace=True) # value == 0: noisy\n\n    df[[\"currentLocationLocationId\", \"homeTownLocationId\"]] = (df[[\"currentLocationLocationId\", \"homeTownLocationId\"]]\n                                                             .applymap(str_normalize).astype(\"category\"))\n    df[object_cols] = df[object_cols].astype('category')\n    \n    return df","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Others"},{"metadata":{"trusted":true},"cell_type":"code","source":"# New feature from columns 63, 64\ndef process_63_64(z):\n    x, y = z\n    if x != x and y != y:\n        return np.nan\n    if (x, y) in [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 8.0), (7.0, 5.0), (5.0, 6.0), (9.0, 43.0), (8.0, 9.0)]: return True\n    else: return False\n    \ndef process_others(df):        \n    df[[\"Field_27\", \"Field_28\"]].replace(0.0, np.nan, inplace=True)\n    df['F18_isnumeric'] = df['Field_18'].str.isnumeric()\n    df['F18_isalpha'] = df['Field_18'].str.isalpha()\n    \n    # Delta from some pairs of columns\n    for i, j in [(20, 27), (28, 27), (39, 41), (41, 42), (50, 51), (51, 53)]:\n        df[f'F{str(i)}_{str(j)}_delta'] = df[f'Field_{str(j)}'] - df[f'Field_{str(i)}']\n    df['F_59_60'] = df['Field_59'] - df['Field_60'] - 2\n    df['F_63_64'] = df[['Field_63', 'Field_64']].apply(process_63_64, axis=1).astype('category')\n    \n    # Mean, std from partnerX columns\n    for i in '1 2 3 4 5'.split():\n        col = [c for c in df.columns if f'partner{i}' in c]\n        df[f'partner{i}_mean'] = df[col].mean(axis=1)\n        df[f'partner{i}_std'] = df[col].std(axis=1)\n\n    # Reference columns\n    columns = set(df.columns).difference(ignore_columns)\n    df['cnt_NaN'] = df[columns].isna().sum(axis=1)\n    df['cnt_True'] = df[columns].applymap(lambda x: isinstance(x, bool) and x).sum(axis=1)\n    df['cnt_False'] = df[columns].applymap(lambda x: isinstance(x, bool) and not x).sum(axis=1)\n\n    # Combinations of auto columns\n    lst_combination = (list(combinations(auto_columns_2, 2)) + list(combinations(auto_columns_3, 2)) + list(combinations(auto_columns_4, 2)))\n    for l, r in lst_combination:\n        for func in 'add subtract divide multiply'.split():\n            df[f'auto_{func}_{l}_{r}'] = getattr(np, func)(df[l], df[r])\n            \n    return df","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4. Combine all parts"},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform(df):\n    df = process_datetime_cols(df)\n    df = process_categorical_cols(df)\n    df = process_others(df)\n    return df.drop(ignore_columns, axis=1)\n\ntrain = transform(train)\ntest = transform(test)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Datetime preprocessing - Add some columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_dates(df):\n    dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n    for date in dates:\n        df[date+'_day'] = df[date].dt.day\n        df[date+'_month'] = df[date].dt.month\n        df[date+'_year'] = df[date].dt.year\n        df[date+'_week'] = df[date].dt.week\n        df[date+'_dayofweek'] = df[date].dt.dayofweek\n    return df","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def days_between_startEnd(df):\n    start_dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']]\n    end_dates = [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n    col = ['F','E','C','G','A']\n    for i in range(5):\n        df[col[i]+'_delta'] = (df[end_dates[i]]-df[start_dates[i]]).dt.total_seconds()/(60*60*24)\n    return df","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_datetime(df):\n    dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\n    for col in dates:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n    return df","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = [f\"{c}_startDate\" for c in ['F','E','C','G','A']] + [f\"{c}_endDate\" for c in ['F','E','C','G','A']]\ndates_columns = ['F_delta','E_delta','C_delta','G_delta','A_delta']\nfor d in dates:\n    dates_columns.append(d+'_day')\n    dates_columns.append(d+'_month')\n    dates_columns.append(d+'_year')\n    dates_columns.append(d+'_week')\n    dates_columns.append(d+'_dayofweek')\ndates_columns","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"['F_delta',\n 'E_delta',\n 'C_delta',\n 'G_delta',\n 'A_delta',\n 'F_startDate_day',\n 'F_startDate_month',\n 'F_startDate_year',\n 'F_startDate_week',\n 'F_startDate_dayofweek',\n 'E_startDate_day',\n 'E_startDate_month',\n 'E_startDate_year',\n 'E_startDate_week',\n 'E_startDate_dayofweek',\n 'C_startDate_day',\n 'C_startDate_month',\n 'C_startDate_year',\n 'C_startDate_week',\n 'C_startDate_dayofweek',\n 'G_startDate_day',\n 'G_startDate_month',\n 'G_startDate_year',\n 'G_startDate_week',\n 'G_startDate_dayofweek',\n 'A_startDate_day',\n 'A_startDate_month',\n 'A_startDate_year',\n 'A_startDate_week',\n 'A_startDate_dayofweek',\n 'F_endDate_day',\n 'F_endDate_month',\n 'F_endDate_year',\n 'F_endDate_week',\n 'F_endDate_dayofweek',\n 'E_endDate_day',\n 'E_endDate_month',\n 'E_endDate_year',\n 'E_endDate_week',\n 'E_endDate_dayofweek',\n 'C_endDate_day',\n 'C_endDate_month',\n 'C_endDate_year',\n 'C_endDate_week',\n 'C_endDate_dayofweek',\n 'G_endDate_day',\n 'G_endDate_month',\n 'G_endDate_year',\n 'G_endDate_week',\n 'G_endDate_dayofweek',\n 'A_endDate_day',\n 'A_endDate_month',\n 'A_endDate_year',\n 'A_endDate_week',\n 'A_endDate_dayofweek']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_df(df):    \n    for col in dates_columns:\n        df[col] = df[col].fillna(df[col].mean())\n    return df","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = to_datetime(train)\ntest = to_datetime(test)\ntrain = split_dates(train)\ntest = split_dates(test)\ntrain = days_between_startEnd(train)\ntest = days_between_startEnd(test)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = impute_df(train)\ntest = impute_df(test)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5. Try Count Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Support catboost modelling\ncat_features = [c for c in train.columns if (train[c].dtype not in [np.float64, np.int64])]\ntrain[cat_features] = train[cat_features].astype(str)\ntest[cat_features] = test[cat_features].astype(str)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the encoder\nt = pd.concat([train, test]).reset_index(drop=True)\ncount_enc = ce.CountEncoder().fit_transform(t[cat_features])\ntt = t.join(count_enc.add_suffix(\"_count\"))\n\nf2_train = tt.loc[tt.index < train.shape[0]]\nf2_test = tt.loc[tt.index >= train.shape[0]]\n\ncolumns = sorted(set(f2_train.columns).intersection(f2_test.columns))\nprint(len(columns))","execution_count":16,"outputs":[{"output_type":"stream","text":"2275\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 3. Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN, TEST = f2_train[columns].drop(['id', 'label'], axis=1), f2_test[columns].drop(['id', 'label'], axis=1)\nLABEL = f2_train['label']\npreds, test_preds, gini = np.zeros(TRAIN.shape[0]), {}, {}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True)\nfor i, (train_idx, val_idx) in enumerate(cv.split(TRAIN, LABEL)):\n    X_train, y_train = TRAIN.iloc[train_idx], LABEL.iloc[train_idx]\n    X_val, y_val = TRAIN.iloc[val_idx], LABEL.iloc[val_idx]\n\n    model = CatBoostClassifier(eval_metric='AUC', \n                             use_best_model=True,\n                             iterations=1000, \n                             learning_rate=0.1, \n                             random_seed=42).fit(X_train, y_train, \n                                                 cat_features=set(cat_features),\n                                                 eval_set=(X_val, y_val), verbose=500)\n\n    y_pred = model.predict(X_val)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n        \n    preds[val_idx] = y_pred_proba\n    test_preds[f'F{i+1}'] = model.predict_proba(TEST)[:, 1]\n    \n    gini[f'F{i+1}'] = 2 * roc_auc_score(y_val, y_pred_proba) - 1\n    \n    \n# Resulting\nroc_auc = roc_auc_score(LABEL, preds)\nprint('Avg GINI score:', 2*roc_auc - 1)\n\nresult = np.array(list(gini.values()))\nprint('GINI: {:.5f} +- {:.5f}'.format(result.mean(), result.std()))","execution_count":17,"outputs":[{"output_type":"stream","text":"0:\ttest: 0.6683206\tbest: 0.6683206 (0)\ttotal: 1.11s\tremaining: 18m 26s\n500:\ttest: 0.7464222\tbest: 0.7464369 (493)\ttotal: 5m 44s\tremaining: 5m 43s\n999:\ttest: 0.7478934\tbest: 0.7481795 (883)\ttotal: 11m 20s\tremaining: 0us\n\nbestTest = 0.7481794759\nbestIteration = 883\n\nShrink model to first 884 iterations.\n0:\ttest: 0.6825500\tbest: 0.6825500 (0)\ttotal: 900ms\tremaining: 14m 59s\n500:\ttest: 0.7577145\tbest: 0.7580026 (475)\ttotal: 5m 41s\tremaining: 5m 40s\n999:\ttest: 0.7549888\tbest: 0.7586250 (511)\ttotal: 11m 14s\tremaining: 0us\n\nbestTest = 0.7586249669\nbestIteration = 511\n\nShrink model to first 512 iterations.\n0:\ttest: 0.6899467\tbest: 0.6899467 (0)\ttotal: 808ms\tremaining: 13m 27s\n500:\ttest: 0.7529570\tbest: 0.7533952 (429)\ttotal: 5m 36s\tremaining: 5m 35s\n999:\ttest: 0.7537689\tbest: 0.7550459 (741)\ttotal: 11m 11s\tremaining: 0us\n\nbestTest = 0.7550459229\nbestIteration = 741\n\nShrink model to first 742 iterations.\n0:\ttest: 0.6855685\tbest: 0.6855685 (0)\ttotal: 818ms\tremaining: 13m 36s\n500:\ttest: 0.7447272\tbest: 0.7448435 (469)\ttotal: 5m 37s\tremaining: 5m 36s\n999:\ttest: 0.7437929\tbest: 0.7454275 (545)\ttotal: 11m 6s\tremaining: 0us\n\nbestTest = 0.7454275461\nbestIteration = 545\n\nShrink model to first 546 iterations.\n0:\ttest: 0.6819341\tbest: 0.6819341 (0)\ttotal: 793ms\tremaining: 13m 12s\n500:\ttest: 0.7496950\tbest: 0.7497981 (498)\ttotal: 5m 37s\tremaining: 5m 36s\n999:\ttest: 0.7507197\tbest: 0.7511542 (788)\ttotal: 11m 7s\tremaining: 0us\n\nbestTest = 0.7511541769\nbestIteration = 788\n\nShrink model to first 789 iterations.\nAvg GINI score: 0.5031479465004816\nGINI: 0.50337 +- 0.00943\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 4. Submisison"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['label'] = pd.DataFrame(test_preds).mean(axis=1).values\ntest[['id', 'label']].to_csv('submission.csv', index=False)","execution_count":18,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}